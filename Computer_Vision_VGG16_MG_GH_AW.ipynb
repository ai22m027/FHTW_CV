{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c6792a-3a15-4cdb-8244-101e8c897600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# use pickle to store data and models https://docs.python.org/3/library/pickle.html \n",
    "\n",
    "# Set True if you dont have the images locally already!\n",
    "reload = False\n",
    "    \n",
    "classes = [\"Sushi\", \"Hot dog\", \"Pizza\"]\n",
    "split = None #\"validation\" # [\"train\", \"test\" or \"validation\"] None --> Read all\n",
    "max_samples = None # 10\n",
    "num_classes = 3\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "label=0\n",
    "\n",
    "if reload:\n",
    "\n",
    "    # Import all datasets \n",
    "    for _class in classes:\n",
    "        # get dataset per class\n",
    "        dataset = fo.zoo.load_zoo_dataset(\n",
    "            \"open-images-v7\", \n",
    "            split=split, \n",
    "            label_types=[\"classifications\"], \n",
    "            classes = _class,\n",
    "            max_samples=max_samples,\n",
    "            seed=None,\n",
    "            shuffle=False,\n",
    "            dataset_name = _class,\n",
    "            dataset_dir = \"datasets/\"+_class,\n",
    "            download_if_necessary = True,\n",
    "            drop_existing_dataset = False,\n",
    "            overwrite = True,\n",
    "            cleanup = False,\n",
    "        )\n",
    "        # get image paths as list\n",
    "        img_paths = dataset.values(\"filepath\")\n",
    "\n",
    "        label += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692f3376",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08317ac4-9b6b-484e-93a9-a4cc02fcdaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Resize Rectangular to quadratic size for all imported images (Size == VGG16 Model Size)\n",
    "size = 224\n",
    "# Batch size\n",
    "batch_size = 64\n",
    "\n",
    "# Convert to Tensorflow Dataset and perform 80/20 split\n",
    "train_ds= tf.keras.utils.image_dataset_from_directory(\n",
    "  directory=os.getcwd()+\"/datasets/\",\n",
    "  image_size=(size, size),\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=1337,\n",
    "  batch_size=batch_size)\n",
    "\n",
    "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  directory=os.getcwd()+\"/datasets/\",\n",
    "  image_size=(size, size),\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=1337,\n",
    "  batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b685c7-3cc7-4b00-87d7-48cc6fdc5c93",
   "metadata": {},
   "source": [
    "## Show some Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d8c422-6f13-48cc-9c65-de86f5ad2f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot 9 Examples including Labels\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_ds.take(9):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(train_ds.class_names[labels[i]])\n",
    "        plt.axis(\"off\")\n",
    "        \n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in test_ds.take(9):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(train_ds.class_names[labels[i]])\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c18281-8cf2-44b4-a886-31b4460da057",
   "metadata": {},
   "source": [
    "## ONE HOT Encoding for Classification Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03259b30-6947-4039-9fb5-f72376a5ce53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHUFFLE DATA BEFORE TRAINING\n",
    "train_ds = train_ds.shuffle(len(train_ds))\n",
    "test_ds = test_ds.shuffle(len(test_ds))\n",
    "\n",
    "# one-hot encode train_ds and test_ds\n",
    "train_ds = train_ds.map(lambda x, y: (x, tf.one_hot(y, num_classes)))\n",
    "\n",
    "# One-hot encode the target labels of test_ds\n",
    "test_ds = test_ds.map(lambda x, y: (x, tf.one_hot(y, num_classes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6028be",
   "metadata": {},
   "source": [
    "# 1. Transfer Learning (Base Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c8085a",
   "metadata": {},
   "source": [
    "### Modify for 3 classes and compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd45ffc1-408d-4678-935c-708a6f2a1439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained VGG16 model\n",
    "vgg_model = tf.keras.applications.VGG16(\n",
    "    weights='imagenet',\n",
    "    include_top=True,\n",
    "    input_shape=(size, size, 3)\n",
    ")\n",
    "\n",
    "# Print model summary\n",
    "vgg_model.summary()\n",
    "\n",
    "# Replace the output layer\n",
    "num_classes = 3  # Number of classes in your dataset\n",
    "output_layer = tf.keras.layers.Dense(num_classes, activation='softmax')(vgg_model.layers[-2].output)\n",
    "\n",
    "# Create the new model\n",
    "model = tf.keras.Model(inputs=vgg_model.input, outputs=output_layer)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339230f5",
   "metadata": {},
   "source": [
    "### Train VGG16 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f464145f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "filepath = r\"Transfer_Learning.h5\"\n",
    "\n",
    "# CALLBACK TO SAVE BEST MODEL\n",
    "save_best = tf.keras.callbacks.ModelCheckpoint(filepath, \n",
    "                                                monitor='val_loss', \n",
    "                                                verbose=1, \n",
    "                                                save_best_only=True,\n",
    "                                                save_weights_only=False, \n",
    "                                                mode='auto', \n",
    "                                                save_freq='epoch')\n",
    "history = model.fit(train_ds,\n",
    "                    validation_data = test_ds, \n",
    "                    epochs=num_epochs, \n",
    "                    verbose = 1,\n",
    "                    shuffle = True,)\n",
    "                    #callbacks=[save_best]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f264e239-b1b0-4234-831c-38d9ad57febd",
   "metadata": {},
   "outputs": [],
   "source": [
    "legend = list(history.history.keys())\n",
    "for key in history.history.keys():\n",
    "    plt.plot(history.history[key][1:])\n",
    "plt.title('Transfer Learning')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(legend, loc='upper right')\n",
    "plt.grid()\n",
    "plt.draw()\n",
    "fig1 = plt.gcf()\n",
    "fig1.set_dpi(200)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41abcc5d",
   "metadata": {},
   "source": [
    "# 2. Transfer Learning + Augmentation  (Base Model with Augmentation) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b142454a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a40dbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Auto-Check Function if Image is Blurry (NOT USED - Basically no blurry Images in dataset)\n",
    "\n",
    "def is_image_blurry(image, threshold):\n",
    "    grayscale_image = tf.image.rgb_to_grayscale(image)\n",
    "    grayscale_image = tf.image.grayscale_to_rgb(grayscale_image)  # Convert to 3 channels\n",
    "    grayscale_image = tf.cast(grayscale_image, tf.float32)  # Convert to float32\n",
    "    grayscale_image = tf.expand_dims(grayscale_image, axis=0)  # Expand dimensions for compatibility\n",
    "    gradients = tf.image.sobel_edges(grayscale_image)\n",
    "    gradient_magnitude = tf.sqrt(tf.reduce_sum(gradients ** 2, axis=-1))\n",
    "    average_gradient = tf.reduce_mean(gradient_magnitude)\n",
    "    is_blurry = tf.less(average_gradient, threshold)\n",
    "    return is_blurry\n",
    "\n",
    "# --> Manuel Cleansing of Data by Human Eye! --> Sort out Senseless Images!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f363e9",
   "metadata": {},
   "source": [
    "## Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4de4d8f-baa2-4833-8e4c-25ea142df65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data augmentation settings\n",
    "\n",
    "print(\"Total Number of Batches for Training BEFORE Augmentation: \", len(list(train_ds.as_numpy_iterator())))\n",
    "\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "    tf.keras.layers.RandomZoom(0.2), #tf.keras.layers.RandomCrop(size,size),\n",
    "    tf.keras.layers.RandomRotation(0.2), \n",
    "    tf.keras.layers.RandomTranslation(0.2, 0.2),\n",
    "])\n",
    "\n",
    "aug_train_ds = train_ds.map(lambda x, y: (data_augmentation(x, training=True), y))\n",
    "aug_test_ds = test_ds.map(lambda x, y: (data_augmentation(x, training=True), y))\n",
    "\n",
    "train_ds = train_ds.concatenate(aug_train_ds)\n",
    "test_ds = test_ds.concatenate(aug_test_ds)\n",
    "\n",
    "print(\"Total Number of Batches for Training AFTER Augmentation: \", len(list(train_ds.as_numpy_iterator())))\n",
    "\n",
    "# SHUFFLE DATA BEFORE TRAINING\n",
    "train_ds = train_ds.shuffle(len(train_ds))\n",
    "test_ds = test_ds.shuffle(len(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f5d624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained VGG16 model\n",
    "vgg_model = tf.keras.applications.VGG16(\n",
    "    weights='imagenet',\n",
    "    include_top=True,\n",
    "    input_shape=(size, size, 3)\n",
    ")\n",
    "\n",
    "# Print model summary\n",
    "vgg_model.summary()\n",
    "\n",
    "# Replace the output layer\n",
    "num_classes = 3  # Number of classes in your dataset\n",
    "output_layer = tf.keras.layers.Dense(num_classes, activation='softmax')(vgg_model.layers[-2].output)\n",
    "\n",
    "# Create the new model\n",
    "model_augmented = tf.keras.Model(inputs=vgg_model.input, outputs=output_layer)\n",
    "\n",
    "# Compile the model\n",
    "model_augmented.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f0e96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "filepath = r\"Transfer_Learning_Cleansed_Augmented.h5\"\n",
    "\n",
    "# CALLBACK TO SAVE BEST MODEL\n",
    "save_best = tf.keras.callbacks.ModelCheckpoint(filepath, \n",
    "                                                monitor='val_loss', \n",
    "                                                verbose=1, \n",
    "                                                save_best_only=True,\n",
    "                                                save_weights_only=False, \n",
    "                                                mode='auto', \n",
    "                                                save_freq='epoch')\n",
    "history_augmented = model_augmented.fit(train_ds,\n",
    "                    validation_data = test_ds, \n",
    "                    epochs=num_epochs, \n",
    "                    verbose = 1,\n",
    "                    shuffle = True)\n",
    "                    #callbacks=[save_best]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d13ebcf-9b72-4cf7-b340-b01f5bc0202b",
   "metadata": {},
   "outputs": [],
   "source": [
    "legend = list(history_augmented.history.keys())\n",
    "for key in history_augmented.history.keys():\n",
    "    plt.plot(history_augmented.history[key][1:])\n",
    "plt.title('Transfer Learning  with Cleansing & Augmentation')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(legend, loc='upper right')\n",
    "plt.grid()\n",
    "plt.draw()\n",
    "fig1 = plt.gcf()\n",
    "fig1.set_dpi(200)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dce783",
   "metadata": {},
   "source": [
    "# 3. Transfer Learning + Cleansing + Augmentation + Architecture (Custom Model (New Architecture))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd76d80",
   "metadata": {},
   "source": [
    "## Modified Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d1256e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freezing layers 1-5\n",
    "for layer in vgg_model.layers[:5]:\n",
    "    layer.trainable = False\n",
    "    \n",
    "output = vgg_model.get_layer('block3_conv3').output\n",
    "\n",
    "conv1 = tf.keras.layers.Conv2D(512,(1,1), padding=\"same\", activation=\"relu\")(output)\n",
    "conv2 = tf.keras.layers.Conv2D(1024,(3,3), padding=\"valid\",activation=\"relu\")(conv1)\n",
    "flatten = tf.keras.layers.Flatten()(conv2)\n",
    "dense = tf.keras.layers.Dense(256,activation=\"relu\")(flatten)\n",
    "final = tf.keras.layers.Dense(num_classes, activation=\"softmax\")(dense)\n",
    "\n",
    "model_custom = tf.keras.Model(inputs=vgg_model.input, outputs=final)\n",
    "model_custom.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af1b8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "filepath = r\"Transfer_Learning_Cleansed_Augmented_newArchitecture.h5\"\n",
    "\n",
    "# CALLBACK TO SAVE BEST MODEL\n",
    "save_best = tf.keras.callbacks.ModelCheckpoint(filepath, \n",
    "                                                monitor='val_loss', \n",
    "                                                verbose=1, \n",
    "                                                save_best_only=True,\n",
    "                                                save_weights_only=False, \n",
    "                                                mode='auto', \n",
    "                                                save_freq='epoch')\n",
    "history_custom = model_custom.fit(train_ds,\n",
    "                    validation_data = test_ds, \n",
    "                    epochs=num_epochs, \n",
    "                    verbose = 1,\n",
    "                    shuffle = True,)\n",
    "                    #callbacks=[save_best])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40a3723-8cc0-47b7-969a-3d48a47da4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "legend = list(history_custom.history.keys())\n",
    "for key in history_custom.history.keys():\n",
    "    plt.plot(history_custom.history[key][1:])\n",
    "plt.title('Transfer Learning  with Cleansing & New Augmentation & Diff. Architecture')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(legend, loc='upper right')\n",
    "plt.grid()\n",
    "plt.draw()\n",
    "fig1 = plt.gcf()\n",
    "fig1.set_dpi(200)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f073a3ae",
   "metadata": {},
   "source": [
    "## Summary of Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaaeac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7e5fa9-1cd6-462e-901e-5a0a9757363a",
   "metadata": {},
   "source": [
    "## Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79052dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base model\n",
    "model.summary()\n",
    "\n",
    "# Get the predicted labels for the test dataset\n",
    "y_pred = model.predict(test_ds)\n",
    "y_pred = np.argmax(y_pred, axis=1)  # Convert predicted probabilities to class labels\n",
    "\n",
    "# Get the true labels for the test dataset\n",
    "y_true = test_ds.map(lambda x, y: y)\n",
    "y_true = np.concatenate(list(y_true.as_numpy_iterator()))\n",
    "y_true = np.argmax(y_true, axis=1)\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Display the confusion matrix\n",
    "print(cm)\n",
    "\n",
    "# Plot the confusion matrix as an image\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c277cca4-e847-4bdb-b2e9-ec7b1eed23fd",
   "metadata": {},
   "source": [
    "## Base Model with Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c04144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base model with augmented data\n",
    "model_augmented.summary()\n",
    "\n",
    "# Get the predicted labels for the test dataset\n",
    "y_pred = model_augmented.predict(test_ds)\n",
    "y_pred = np.argmax(y_pred, axis=1)  # Convert predicted probabilities to class labels\n",
    "\n",
    "# Get the true labels for the test dataset\n",
    "y_true = test_ds.map(lambda x, y: y)\n",
    "y_true = np.concatenate(list(y_true.as_numpy_iterator()))\n",
    "y_true = np.argmax(y_true, axis=1)\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Display the confusion matrix\n",
    "print(cm)\n",
    "\n",
    "# Plot the confusion matrix as an image\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3f89ad-004f-4b65-8729-c0a07b8a7d42",
   "metadata": {},
   "source": [
    "## Custom Model (New Architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2beca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modifed model with augmented data\n",
    "model_custom.summary()\n",
    "\n",
    "# Get the predicted labels for the test dataset\n",
    "y_pred = model_custom.predict(test_ds)\n",
    "y_pred = np.argmax(y_pred, axis=1)  # Convert predicted probabilities to class labels\n",
    "\n",
    "# Get the true labels for the test dataset\n",
    "y_true = test_ds.map(lambda x, y: y)\n",
    "y_true = np.concatenate(list(y_true.as_numpy_iterator()))\n",
    "y_true = np.argmax(y_true, axis=1)\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Display the confusion matrix\n",
    "print(cm)\n",
    "\n",
    "# Plot the confusion matrix as an image\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9367fb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
